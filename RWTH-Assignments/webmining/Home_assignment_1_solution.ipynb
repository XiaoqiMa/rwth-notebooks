{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the links from document\n",
    "import requests\n",
    "api_url = 'https://en.wikipedia.org/w/api.php' #base URL for the Web API of the English Wikipedia\n",
    "\n",
    "some_params={'action': 'query',\n",
    "            'titles': 'Web design',\n",
    "            'prop': 'links', \n",
    "            'pllimit':'50',  # setting the number of links to 50.. can be replaced by any number\n",
    "            'format': 'json'}\n",
    "result = requests.get(url=api_url, params=some_params).json()\n",
    "links = result['query']['pages']['34035']['links']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sandipansikdar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stopwords removal and stemming...\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(exintro):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = exintro.split(' ')\n",
    "    article = ''\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            article += w + ' '\n",
    "    return article.strip()        \n",
    "    \n",
    "def stemming(exintro):\n",
    "    ps = PorterStemmer()\n",
    "    words = exintro.split(' ')\n",
    "    article = ''\n",
    "    for w in words:\n",
    "        w_ = ps.stem(w)\n",
    "        article += w_+' '\n",
    "    return article.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract intro section, perform stopword removal and stemming and store them in respective files in the directory 'docs'\n",
    "\n",
    "for l in links:\n",
    "    title = l['title']\n",
    "    params ={'action': 'query','titles': title,'prop': 'extracts','exintro': 'True','explaintext': 'True', 'format': 'json'}\n",
    "    request = requests.get(url=api_url, params = params).json()\n",
    "    page_id = list(request['query']['pages'].keys())[0]\n",
    "    extract = request['query']['pages'][page_id]['extract']\n",
    "    extract = remove_stopwords(extract)\n",
    "    extract = stemming(extract)\n",
    "    if len(extract)>0:\n",
    "        with open('docs/'+title.replace('.',''),'w') as ft:\n",
    "            ft.write(extract) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain term-frequency matrix\n",
    "import numpy as np\n",
    "import os\n",
    "word2index = {}\n",
    "document2index = {}\n",
    "index2document = {}\n",
    "document_word_vectors = {}\n",
    "w_cnt = 0\n",
    "d_cnt = 0\n",
    "for root, dirs, files in os.walk('docs'):\n",
    "    for f in files:\n",
    "        document_word_vectors[f] = []\n",
    "        document2index[f] = d_cnt\n",
    "        index2document[d_cnt] = f\n",
    "        d_cnt+=1\n",
    "        with open(root+'/'+f) as fs:\n",
    "            try:\n",
    "                for line in fs:\n",
    "                    words = line.strip().split()\n",
    "                    for w in words:\n",
    "                        if w not in word2index:\n",
    "                            word2index[w] = w_cnt\n",
    "                            w_cnt+=1\n",
    "                        document_word_vectors[f].append(word2index[w])\n",
    "            except:\n",
    "                print (f)\n",
    "                        \n",
    "w_f_matrix = np.zeros((len(word2index),len(document2index)))\n",
    "for doc in document_word_vectors:\n",
    "    i = document2index[doc]\n",
    "    for j in document_word_vectors[doc]:\n",
    "        w_f_matrix[j,i]+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 1., ..., 2., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.],\n",
       "       [6., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_f_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocssing the query vector\n",
    "q = 'web development design'\n",
    "q = remove_stopwords(q)\n",
    "q = stemming(q)\n",
    "query = []\n",
    "q_v = np.zeros(len(word2index))\n",
    "for w in q.split():\n",
    "    q_v[word2index[w]]+=1\n",
    "    query.append(word2index[w])\n",
    "max_ = np.max(q_v)\n",
    "def normalize_query(i,max_):\n",
    "    return 0.5+(0.5*i)/max_\n",
    "norm_q = np.vectorize(normalize_query)\n",
    "q_v = norm_q(q_v,max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the similarity function\n",
    "from scipy import spatial\n",
    "def similarity(l_1,l_2):\n",
    "    return 1 - spatial.distance.cosine(l_1, l_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain normalized term-frequency matrix\n",
    "t_f = np.copy(w_f_matrix)\n",
    "max_f = np.zeros(len(document2index))\n",
    "for i in range(len(document2index)):\n",
    "    max_f[i] = np.max(t_f[:,i])\n",
    "t_f = np.divide(t_f,max_f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_k(k,doc_sim,index2document):\n",
    "    for doc,sim in sorted(doc_sim.items(),key = lambda x:x[1], reverse=True):\n",
    "        print (index2document[doc])\n",
    "        k-=1\n",
    "        if k==0:\n",
    "            break\n",
    "def find_k_relevant_documents(t_f,q_v,k,index2document):\n",
    "    doc_sim = {}\n",
    "    for i in range(t_f.shape[1]):\n",
    "        sim = similarity(t_f[:,i],q_v)\n",
    "        doc_sim[i] = sim\n",
    "    find_top_k(k,doc_sim,index2document)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog\n",
      "Acid3\n",
      "Architect-led design–build\n",
      "Advertising\n",
      "CERN\n"
     ]
    }
   ],
   "source": [
    "# top 5 documents based on only term frequency\n",
    "find_k_relevant_documents(t_f,q_v,5,index2document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining tf-idf matrix\n",
    "inv_doc_freq = np.count_nonzero(t_f,axis=1)\n",
    "def normalize(a,x):\n",
    "    return np.log(x/a)\n",
    "norm = np.vectorize(normalize)\n",
    "inv_doc_freq = norm(inv_doc_freq,len(document2index))\n",
    "tf_idf = np.multiply(t_f,inv_doc_freq.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16762898, 0.        , 0.20953622, ..., 0.20953622, 0.        ,\n",
       "        0.        ],\n",
       "       [0.7613325 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [1.522665  , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.95166562],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.95166562],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.95166562]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the query\n",
    "q_v = np.multiply(q_v,inv_doc_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog\n",
      "Advertising\n",
      "Architect-led design–build\n",
      "Chartered Society of Designers\n",
      "Acid3\n"
     ]
    }
   ],
   "source": [
    "# top 5 documents by tf-idf\n",
    "find_k_relevant_documents(tf_idf,q_v,5,index2document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical language model\n",
    "t_f = np.copy(w_f_matrix)\n",
    "def statistical_language_model(query, t_f, k, index2document):\n",
    "    doc_similarity = {}\n",
    "    for i in range(t_f.shape[1]):\n",
    "        w_sum = np.sum(t_f[:,i])\n",
    "        sim = 1\n",
    "        for w in query:\n",
    "            sim*=t_f[w,i]/w_sum\n",
    "        doc_similarity[i] = sim\n",
    "    find_top_k(k,doc_similarity,index2document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acid1\n",
      "Acid2\n",
      "Chartered Society of Designers\n",
      "Body text\n",
      "Affective design\n"
     ]
    }
   ],
   "source": [
    "# top 5 documents by statistical language model\n",
    "statistical_language_model(query, t_f,5,index2document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent semantic indexing (LSI)\n",
    "from numpy import linalg\n",
    "def latent_semantic_indexing(query, t_f, k, topk, index2document):\n",
    "    doc_similarity = {}\n",
    "    u,s,v = linalg.svd(t_f)\n",
    "    u = u[:,:k]\n",
    "    s_ = np.zeros((k,k))\n",
    "    for i in range(k):\n",
    "        s_[i,i] = s[i]\n",
    "    v = v[:k,:]\n",
    "    q_v = np.zeros(len(word2index))\n",
    "    for q in query:\n",
    "        q_v[q]+=1\n",
    "    q_v = q_v.reshape(1,-1)\n",
    "    q_v = np.matmul(q_v,u)\n",
    "    s_ = linalg.inv(s_)\n",
    "    q_v = np.matmul(q_v,s_)\n",
    "    \n",
    "    for i in range(v.shape[1]):\n",
    "        sim = similarity(q_v,v[:,i])\n",
    "        doc_similarity[i] = sim \n",
    "    find_top_k(topk, doc_similarity,index2document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blueprint\n",
      "Color theory\n",
      "Algorithms-Aided Design (AAD)\n",
      "Adaptive web design\n",
      "Boiler design\n"
     ]
    }
   ],
   "source": [
    "# top 5 documents by statistical language model (LSI)\n",
    "latent_semantic_indexing(query, t_f, 3, 5, index2document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
