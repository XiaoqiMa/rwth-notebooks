{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract related information about users in 'The_Donald' and 'politics' subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "\n",
    "def cleanComm(comment):\n",
    "    comment = comment.replace('\\n', '[lf}')\n",
    "    comment = comment.replace('\\r', '[cr}')\n",
    "    comment = comment.replace('\\t', '[ts}')\n",
    "    comment = comment.replace('\\\\', '[2s}')\n",
    "    comment = comment.replace('\\'', '[ap}')\n",
    "    comment = comment.replace('\\\"', '[dap}')\n",
    "    return comment\n",
    "\n",
    "def extractInformation(file_path, subreddit):\n",
    "    start_time = time.process_time()\n",
    "    # Get the total lines of the datasets, in order to read data into segementations\n",
    "    with open(filename, 'r') as reddit_file:\n",
    "        reddit_data = reddit_file.readlines()\n",
    "        total_lines = len(reddit_data)\n",
    "#     total_lines = 62558772\n",
    "    curr_line = 0\n",
    "    LIMIT = 2000000 # Group data with each 2 million lines\n",
    "    line_count = LIMIT\n",
    "    lines = []\n",
    "    progress = 0 # To mark the process proportion\n",
    "    with open(file_path, 'r+') as reddit_file: \n",
    "        for line in reddit_file:\n",
    "            if line:\n",
    "                try:\n",
    "                    jsn = json.loads(line)\n",
    "                    lines.append(jsn)\n",
    "                    curr_line += 1\n",
    "                except ValueError as err:\n",
    "                    pass\n",
    "            if curr_line == line_count: # Create a dictory to save processing data if not exist\n",
    "                if not os.path.exists(subreddit):\n",
    "                    os.makedirs(subreddit)\n",
    "    \n",
    "                filename = './{0}/data_{1}.csv'.format(subreddit, str(math.ceil(curr_line / LIMIT)))\n",
    "                index_len = len(lines)\n",
    "                #Convert JSON format file to DataFrame format\n",
    "                df = pd.DataFrame([line for line in lines], index=[i for i in range(index_len)])\n",
    "                # Specify the subreddit and some related information\n",
    "                df = df[df['subreddit'] == subreddit]\n",
    "                df_comm = df.loc[df['author'] != '[deleted]', ['author', 'score', 'body', 'link_id', 'created_utc']]\n",
    "\n",
    "                senti_score_list = []\n",
    "                for i in range(len(df_comm)):\n",
    "                    score = 0\n",
    "                    statements = df_comm.iat[i, 2] # locate the comment body\n",
    "                    sentiment = TextBlob(statements) # Use TextBlob library to calculate the polarity score\n",
    "                    scores = sentiment.sentiment.polarity\n",
    "                    subjectivity = sentiment.sentiment.subjectivity\n",
    "                    senti_score_list.append(scores)\n",
    "                    df_comm.iat[i, 2] = cleanComm(df_comm.iat[i, 2]) # After obtaining the score, santinize the comment\n",
    "                    \n",
    "                df_comm['senti_score'] = pd.Series(senti_score_list, index=df_comm.index)\n",
    "                # Write into csv format file for secondary processing\n",
    "                df_comm.to_csv(filename, sep='\\t', encoding='utf-8')\n",
    "                \n",
    "                if line_count + LIMIT < total_lines:\n",
    "                    line_count += LIMIT\n",
    "                else:\n",
    "                    line_count = total_lines # Reset line count and starts new round\n",
    "                lines = [] # Reset list to save data\n",
    "                # Monitor the processing\n",
    "                progress = curr_line\n",
    "                print('{0} of {1} lines read ({2}%)'.format(progress, total_lines, int(progress / total_lines * 100)))\n",
    "                end_time = time.process_time()\n",
    "                print('running time: ', (end_time-start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = './cleands2'\n",
    "# extractInformation(file_path, 'The_Donald')\n",
    "# extractInformation(file_path, 'politics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract highly scored author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractHighScoreAuth(file_path):\n",
    "    # Use preprocessing data; list all files in specific folder\n",
    "    files = os.listdir(file_path)\n",
    "    file_count = len(files)\n",
    "    count = 1\n",
    "    process = 0\n",
    "    for file in files:\n",
    "        filename = file_path + file\n",
    "        author_score_count = {}\n",
    "        try:\n",
    "            # Read csv file\n",
    "            df_author_score = pd.read_csv(filename, sep='\\t')\n",
    "            # Read all rows with designated columns: author, score. Sort by score\n",
    "            df_author_score = df_author_score.loc[:, ['author', 'score']]\n",
    "            df_author_score = df_author_score.sort_values('score', ascending=False)\n",
    "\n",
    "            # Calculate sum of scores by the same author\n",
    "            for author in df_author_score['author'].unique():\n",
    "                score_sum = df_author_score.loc[df_author_score['author'] == author, ['score']].sum()\n",
    "                if author not in author_score_count.keys():\n",
    "                    author_score_count[author] = int(score_sum)\n",
    "                else:\n",
    "                    author_score_count[author] += int(score_sum)\n",
    "            # Convert to the DataFrame with authors and total socres, sort by scores\n",
    "            df_scores = pd.DataFrame(author_score_count, index=['scores'], dtype='int64')\n",
    "            df_scores = df_scores.transpose()\n",
    "            df_scores = df_scores.sort_values('scores', ascending=False)\n",
    "            \n",
    "            # Filter those authors whose total obtained scores ranked top 200 in each separated file\n",
    "            if len(df_scores) > 200:\n",
    "                df_scores = df_scores[:200]\n",
    "            \n",
    "            # Generate filename and write into csv format file\n",
    "            folderName = './' + filepath.split('/')[1] + '_highScore/'\n",
    "            if not os.path.exists(folderName):\n",
    "                    os.makedirs(folderName)\n",
    "            to_file_name = '{0}highScore_{1}.csv'.format(folderName, count)\n",
    "            df_scores.to_csv(to_file_name, sep='\\t')\n",
    "            print('Processing {0}/{1}'.format(count, file_count))\n",
    "            count += 1\n",
    "            author_score_count = {}\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = './The_Donald/'\n",
    "# extractHighScoreAuth(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = './politics/'\n",
    "# extractHighScoreAuth(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract highly linked author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "def extractHighComm(file_path):\n",
    "    # Use preprocessing data; list all files in specific folder\n",
    "    files = os.listdir(file_path)\n",
    "    file_count = len(files)\n",
    "    count = 1\n",
    "    process = 0\n",
    "    for file in files:\n",
    "        try:\n",
    "            filename = file_path + file\n",
    "            author_comm_count = {}\n",
    "            # Read all rows with designated columns: author, link_id, created_utc\n",
    "            df_author_comm = pd.read_csv(filename, sep='\\t')\n",
    "            df_author_comm = df_author_comm.loc[:, ['author', 'link_id', 'created_utc']]\n",
    "            link_id_count = df_author_comm['link_id'].value_counts()\n",
    "            for link_id in link_id_count.index:\n",
    "                # Find out the author who submit the comment, which is assumed to be the top level comment\n",
    "                author = df_author_comm.loc[df_author_comm['link_id'] == link_id].sort_values('created_utc').iat[0,0]\n",
    "                if author in author_comm_count.keys():\n",
    "                    author_comm_count[author] += link_id_count[link_id]\n",
    "                else:\n",
    "                    author_comm_count[author] = link_id_count[link_id]\n",
    "            \n",
    "            # Convert to the DataFrame with authors and total quotations, sort by number of quotations\n",
    "            df_comm = pd.DataFrame(author_comm_count, index=['comm_below'])\n",
    "            df_comm = df_comm.transpose().sort_values('comm_below', ascending=False)\n",
    "            # Filter the top 200 most frequently linked authors\n",
    "            if len(df_comm) > 200:\n",
    "                df_comm = df_comm[:200]\n",
    "            \n",
    "            # Generate filename and write into csv format file\n",
    "            folderName = './' + filepath.split('/')[1] + '_highlinked/'\n",
    "            if not os.path.exists(folderName):\n",
    "                    os.makedirs(folderName)\n",
    "            to_file_name = '{0}highLinked_{1}.csv'.format(folderName, count)\n",
    "            df_comm.to_csv(to_file_name, sep='\\t')\n",
    "            # Monitor the processing\n",
    "            print('Processing {0}/{1}'.format(count, file_count))\n",
    "            count += 1\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = './The_Donald/'\n",
    "# extractHighComm(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = './politics/'\n",
    "# extractHighComm(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
