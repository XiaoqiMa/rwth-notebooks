{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home assignment 2\n",
    "\n",
    "You should work on the assignement in groups of 2 participants. \n",
    "\n",
    "Upload your solution as a jupyter notebook to L2P by 26th of June 23:59h. (The deadline is strict)\n",
    "\n",
    "Do not forget to specify the names of all contributing students in the jupyter notebook.\n",
    "\n",
    "You should add comments to your code where necessary and print the relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xiaoqi Ma (383420)\n",
    "Anna Wegmann (330509)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dynamic PageRank\n",
    "Consider a random walk setting where the transistion matrix changes over time. At any point of time the probability of a random surfer to jump to a linked page is proportional to the number of previous visits. To start with all the pages are equally likely to be chosen but as the walk continues and the nodes are visited the transition probability changes as proportional to number of previous visits. For example let a page 'a' is linked to pages 'b', 'c' and 'd'. The random surfer currently resides at 'a' and the pages 'b', 'c' and 'd' have already been visited 5, 3 and 2 times respectively. The transition probability would be 0.5, 0.3 and 0.2 respectively. As a new node is viited the probabilities change. The random surfer continues to surf with probability 0.8. Generate 100 random walks and rank the nodes based on the frequency of visit. The random walk should be performed on a drected Erdos-Renyi graph with number of nodes n=200 and probability of edge creation p = 0.4. \n",
    "\n",
    "Hint: Use networkx library for generating graph.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Create a directed Erdos-Renyi graph with 200 nodes, prob of edge creation is p=0.4\n",
    "G = nx.erdos_renyi_graph(200, 0.4, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each random walk, execute randon_surfer function with probability 0.8 to continue\n",
    "# executesd one step for a random surfer, returns the current node and the visit frequency\n",
    "def random_surfer(curr_node, visit_freq):\n",
    "    node_prob = {} # dict to store probability for each node to be visited\n",
    "    freq_sum = 0 # the sum of total visit frequency of other nodes that connect to curr_node\n",
    "    nbrs = list(G.neighbors(curr_node)) # List all connected neighbors of current node\n",
    "    if len(nbrs) > 0:\n",
    "        for nb in nbrs: # for all neighbors\n",
    "            node_prob[nb] = visit_freq[nb]\n",
    "            freq_sum += visit_freq[nb]\n",
    "        for k,v in node_prob.items():\n",
    "            # To get each node probability to be visited\n",
    "            node_prob[k] = v / freq_sum \n",
    "        # randomly choose the next node according to the probability\n",
    "        # random.choice(a,size=None,replace=True,p=None) Generates a random sample from a given 1-D array\n",
    "        next_node = np.random.choice(nbrs, 1, node_prob)[0] \n",
    "        visit_freq[next_node] += 1  \n",
    "        #print('next node: ', next_node)\n",
    "    return (next_node, visit_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stop probability because the surfer continues to surf with prob 0.8\n",
    "p_stop = 0.2\n",
    "visit_freq = {}\n",
    "nodes = list(G.nodes())\n",
    "# Initialize with all nodes as 'visited once' since it is devided by sum of freq in random_surfer\n",
    "for n in nodes:\n",
    "    visit_freq[n] = 1\n",
    "\n",
    "walks = 100\n",
    "for i in range(walks):\n",
    "    # randomly choose a start node\n",
    "    cur_node = random.choice(nodes)\n",
    "    while random.uniform(0, 1) >= p_stop:\n",
    "        # Execute random_surfer function\n",
    "        #print('random surfer: ',i, ' going to node ', cur_node)\n",
    "        (cur_node, visit_freq) = random_surfer(cur_node, visit_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node:  80  visit frequency:  8\n",
      "node:  92  visit frequency:  8\n",
      "node:  47  visit frequency:  6\n",
      "node:  68  visit frequency:  6\n",
      "node:  17  visit frequency:  5\n"
     ]
    }
   ],
   "source": [
    "# Rank the nodes based on the visit frequency\n",
    "counter = 0\n",
    "for n,v in sorted(visit_freq.items(),key = lambda x:x[1], reverse = True): \n",
    "    print('node: ', n, ' visit frequency: ', v-1) # actual frequency is visit_freq-1\n",
    "    counter += 1\n",
    "    # Only output the 5 most frequently visited nodes\n",
    "    if(counter == 5):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node 80: 8 \n",
      "node 92: 8 \n",
      "node 47: 6 \n",
      "node 68: 6 \n",
      "node 17: 5 \n",
      "node 27: 5 \n",
      "node 46: 5 \n",
      "node 57: 5 \n",
      "node 66: 5 \n",
      "node 75: 5 \n",
      "node 78: 5 \n",
      "node 117: 5 \n",
      "node 138: 5 \n",
      "node 155: 5 \n",
      "node 160: 5 \n",
      "node 173: 5 \n",
      "node 3: 4 \n",
      "node 5: 4 \n",
      "node 33: 4 \n",
      "node 44: 4 \n",
      "node 49: 4 \n",
      "node 53: 4 \n",
      "node 56: 4 \n",
      "node 63: 4 \n",
      "node 69: 4 \n",
      "node 81: 4 \n",
      "node 84: 4 \n",
      "node 89: 4 \n",
      "node 105: 4 \n",
      "node 113: 4 \n",
      "node 135: 4 \n",
      "node 143: 4 \n",
      "node 154: 4 \n",
      "node 190: 4 \n",
      "node 192: 4 \n",
      "node 6: 3 \n",
      "node 11: 3 \n",
      "node 12: 3 \n",
      "node 15: 3 \n",
      "node 20: 3 \n",
      "node 24: 3 \n",
      "node 32: 3 \n",
      "node 40: 3 \n",
      "node 41: 3 \n",
      "node 51: 3 \n",
      "node 61: 3 \n",
      "node 70: 3 \n",
      "node 71: 3 \n",
      "node 76: 3 \n",
      "node 77: 3 \n",
      "node 86: 3 \n",
      "node 87: 3 \n",
      "node 121: 3 \n",
      "node 123: 3 \n",
      "node 127: 3 \n",
      "node 128: 3 \n",
      "node 132: 3 \n",
      "node 133: 3 \n",
      "node 137: 3 \n",
      "node 147: 3 \n",
      "node 157: 3 \n",
      "node 158: 3 \n",
      "node 163: 3 \n",
      "node 179: 3 \n",
      "node 180: 3 \n",
      "node 185: 3 \n",
      "node 186: 3 \n",
      "node 191: 3 \n",
      "node 4: 2 \n",
      "node 7: 2 \n",
      "node 8: 2 \n",
      "node 9: 2 \n",
      "node 13: 2 \n",
      "node 16: 2 \n",
      "node 18: 2 \n",
      "node 21: 2 \n",
      "node 22: 2 \n",
      "node 31: 2 \n",
      "node 34: 2 \n",
      "node 37: 2 \n",
      "node 42: 2 \n",
      "node 54: 2 \n",
      "node 60: 2 \n",
      "node 62: 2 \n",
      "node 85: 2 \n",
      "node 90: 2 \n",
      "node 101: 2 \n",
      "node 103: 2 \n",
      "node 104: 2 \n",
      "node 108: 2 \n",
      "node 109: 2 \n",
      "node 119: 2 \n",
      "node 125: 2 \n",
      "node 126: 2 \n",
      "node 129: 2 \n",
      "node 131: 2 \n",
      "node 141: 2 \n",
      "node 145: 2 \n",
      "node 146: 2 \n",
      "node 156: 2 \n",
      "node 159: 2 \n",
      "node 161: 2 \n",
      "node 162: 2 \n",
      "node 164: 2 \n",
      "node 166: 2 \n",
      "node 167: 2 \n",
      "node 169: 2 \n",
      "node 170: 2 \n",
      "node 172: 2 \n",
      "node 174: 2 \n",
      "node 175: 2 \n",
      "node 177: 2 \n",
      "node 178: 2 \n",
      "node 181: 2 \n",
      "node 187: 2 \n",
      "node 188: 2 \n",
      "node 189: 2 \n",
      "node 194: 2 \n",
      "node 196: 2 \n",
      "node 199: 2 \n",
      "node 0: 1 \n",
      "node 1: 1 \n",
      "node 2: 1 \n",
      "node 10: 1 \n",
      "node 14: 1 \n",
      "node 23: 1 \n",
      "node 25: 1 \n",
      "node 28: 1 \n",
      "node 29: 1 \n",
      "node 36: 1 \n",
      "node 39: 1 \n",
      "node 45: 1 \n",
      "node 48: 1 \n",
      "node 50: 1 \n",
      "node 52: 1 \n",
      "node 58: 1 \n",
      "node 64: 1 \n",
      "node 73: 1 \n",
      "node 74: 1 \n",
      "node 79: 1 \n",
      "node 83: 1 \n",
      "node 88: 1 \n",
      "node 91: 1 \n",
      "node 93: 1 \n",
      "node 94: 1 \n",
      "node 95: 1 \n",
      "node 96: 1 \n",
      "node 97: 1 \n",
      "node 99: 1 \n",
      "node 100: 1 \n",
      "node 102: 1 \n",
      "node 107: 1 \n",
      "node 111: 1 \n",
      "node 112: 1 \n",
      "node 114: 1 \n",
      "node 115: 1 \n",
      "node 116: 1 \n",
      "node 120: 1 \n",
      "node 122: 1 \n",
      "node 124: 1 \n",
      "node 130: 1 \n",
      "node 134: 1 \n",
      "node 136: 1 \n",
      "node 139: 1 \n",
      "node 140: 1 \n",
      "node 148: 1 \n",
      "node 149: 1 \n",
      "node 151: 1 \n",
      "node 152: 1 \n",
      "node 153: 1 \n",
      "node 165: 1 \n",
      "node 171: 1 \n",
      "node 182: 1 \n",
      "node 183: 1 \n",
      "node 184: 1 \n",
      "node 195: 1 \n",
      "node 198: 1 \n",
      "node 19: 0 \n",
      "node 26: 0 \n",
      "node 30: 0 \n",
      "node 35: 0 \n",
      "node 38: 0 \n",
      "node 43: 0 \n",
      "node 55: 0 \n",
      "node 59: 0 \n",
      "node 65: 0 \n",
      "node 67: 0 \n",
      "node 72: 0 \n",
      "node 82: 0 \n",
      "node 98: 0 \n",
      "node 106: 0 \n",
      "node 110: 0 \n",
      "node 118: 0 \n",
      "node 142: 0 \n",
      "node 144: 0 \n",
      "node 150: 0 \n",
      "node 168: 0 \n",
      "node 176: 0 \n",
      "node 193: 0 \n",
      "node 197: 0 \n"
     ]
    }
   ],
   "source": [
    "# Output the sorted visit frequency dictionary\n",
    "for m,n in sorted(visit_freq.items(),key = lambda x:x[1], reverse = True): \n",
    "    print('node {0}: {1} '.format(m, n-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Recommendation\n",
    "a. Compare the recommendation algorithms (SVD, NMF, Baseline, k-NN and Random) available in surprise package on movielens dataset in terms of RMSE and MAE.\n",
    "\n",
    "b. Consider the movielens dataset and divide it into (i) training set with 50% of the data (train the algorithms on this part) and (ii) 25% validation set and (iii) test set with the rest. Estimate the ratings of the test set using the algorithms (same as in a) provided by the package on the training set. Your final rating should be weighted average of the ratings predicted by the algorithms. The weights should be learnt on the validation set. Performance should be measured in terms of RMSE.\n",
    "\n",
    "Hint: Use grid search/step-wise update like SGD for learning the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 2a: Compare recommendation algorithms on movielens dataset (here: use movielens-100k)\n",
    "# RMSE: Root Mean Square error (higher weight to large errors)\n",
    "# MAE: Mean absolute error\n",
    "# oriented at example in surprise package: http://surpriselib.com/\n",
    "# algorithms: https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html?highlight=k-NN\n",
    "\n",
    "# load Data\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate\n",
    "data = Dataset.load_builtin('ml-100k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9320  0.9330  0.9440  0.9355  0.9295  0.9348  0.0050  \n",
      "MAE (testset)     0.7335  0.7362  0.7449  0.7368  0.7339  0.7370  0.0041  \n",
      "Fit time          4.79    4.90    4.85    5.16    4.83    4.91    0.13    \n",
      "Test time         2.47    0.14    0.16    0.19    0.19    0.63    0.92    \n"
     ]
    }
   ],
   "source": [
    "algo = SVD()\n",
    "resultSVD = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD on movielens-100k in different runs: RMSE ~ 0.937, MAE ~ 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Estimating biases using als...\n",
      "Evaluating RMSE, MAE of algorithm BaselineOnly on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9468  0.9456  0.9382  0.9476  0.9410  0.9438  0.0036  \n",
      "MAE (testset)     0.7513  0.7486  0.7444  0.7499  0.7472  0.7483  0.0024  \n",
      "Fit time          0.22    0.24    0.25    0.24    0.25    0.24    0.01    \n",
      "Test time         0.16    0.11    0.15    0.15    0.16    0.15    0.02    \n"
     ]
    }
   ],
   "source": [
    "from surprise import BaselineOnly\n",
    "algo = BaselineOnly()\n",
    "resultBaseline = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline on movielens-100k in different runs: RMSE ~ 0.94, MAE ~ 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm NMF on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9631  0.9601  0.9614  0.9609  0.9672  0.9625  0.0025  \n",
      "MAE (testset)     0.7548  0.7574  0.7556  0.7560  0.7586  0.7565  0.0013  \n",
      "Fit time          5.21    5.19    5.56    5.38    5.19    5.30    0.15    \n",
      "Test time         0.14    0.12    0.12    0.17    0.17    0.14    0.02    \n"
     ]
    }
   ],
   "source": [
    "from surprise import NMF\n",
    "algo = NMF()\n",
    "resultNMF = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF on movielens-100k in different runs: RMSE ~ 0.96, MAE ~ 0.76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Evaluating RMSE, MAE of algorithm KNNBasic on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.9851  0.9830  0.9764  0.9772  0.9747  0.9793  0.0040  \n",
      "MAE (testset)     0.7789  0.7762  0.7716  0.7705  0.7707  0.7736  0.0034  \n",
      "Fit time          0.41    0.44    0.47    0.44    0.44    0.44    0.02    \n",
      "Test time         3.51    3.30    3.39    3.32    3.40    3.38    0.08    \n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNBasic\n",
    "algo = KNNBasic()\n",
    "resultKNN = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-NN on movielens-100k in different runs: RMSE ~ 0.98, MAE ~ 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm NormalPredictor on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.5214  1.5200  1.5243  1.5069  1.5194  1.5184  0.0060  \n",
      "MAE (testset)     1.2219  1.2177  1.2227  1.2119  1.2163  1.2181  0.0040  \n",
      "Fit time          0.11    0.13    0.15    0.14    0.15    0.13    0.02    \n",
      "Test time         0.13    0.14    0.18    0.19    0.18    0.16    0.03    \n"
     ]
    }
   ],
   "source": [
    "from surprise import NormalPredictor\n",
    "algo = NormalPredictor()\n",
    "resultRandom = cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random on movielens-100k in different runs: RMSE ~ 1.52, MAE ~ 1.22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the lower the RMSE/MAE values are the less errors there are in the algorithms: algorithms are already sorted from best to worst (SVD, Baseline, NMF, k-NN and Random) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 2b (data: user,movie,rating (1-5) )\n",
    "# Divide Dataset into: 50% training set, 25% validation set, 25% test set??\n",
    "from surprise.model_selection import train_test_split\n",
    "trainset, valTestset = train_test_split(data, train_size=0.5)\n",
    "validationset = valTestset[0:int(len(valTestset)/2)]\n",
    "testset = valTestset[int(len(valTestset)/2):len(valTestset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.baseline_only.BaselineOnly at 0x7f9e354de3c8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train algorithms on training set\n",
    "from surprise import NMF, KNNBasic, SVD, NormalPredictor, BaselineOnly\n",
    "svdAlgo = SVD()\n",
    "nmfAlgo = NMF()\n",
    "randomAlgo = NormalPredictor()\n",
    "kNNAlgo = KNNBasic()\n",
    "baselineAlgo = BaselineOnly()\n",
    "\n",
    "svdAlgo.fit(trainset)\n",
    "nmfAlgo.fit(trainset)\n",
    "randomAlgo.fit(trainset)\n",
    "kNNAlgo.fit(trainset)\n",
    "baselineAlgo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9570\n",
      "RMSE: 0.9860\n",
      "RMSE: 1.5075\n",
      "RMSE: 0.9990\n",
      "RMSE: 0.9495\n"
     ]
    }
   ],
   "source": [
    "# calculate ratings on the validationset (here: we output the accuracy on this as well)\n",
    "from surprise import accuracy\n",
    "\n",
    "validationSVD = svdAlgo.test(validationset)\n",
    "svdAccuracy = accuracy.rmse(validationSVD)\n",
    "\n",
    "validationNMF = nmfAlgo.test(validationset)\n",
    "nmfAccuracy = accuracy.rmse(validationNMF)\n",
    "\n",
    "validationRandom = randomAlgo.test(validationset)\n",
    "randomAccuracy = accuracy.rmse(validationRandom)\n",
    "\n",
    "validationKNN = kNNAlgo.test(validationset)\n",
    "kNNAccuracy = accuracy.rmse(validationKNN)\n",
    "\n",
    "validationBaseline = baselineAlgo.test(validationset)\n",
    "baselineAccuracy = accuracy.rmse(validationBaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Matrix of the estimated ratings of the validationset - columns are the different algos\n",
    "import numpy as np\n",
    "svdEst = [x.est for x in validationSVD]\n",
    "nmfEst = [x.est for x in validationNMF]\n",
    "randomEst = [x.est for x in validationRandom]\n",
    "kNNEst = [x.est for x in validationKNN]\n",
    "baselineEst = [x.est for x in validationBaseline]\n",
    "\n",
    "correctRating = np.transpose([x.r_ui for x in validationSVD])\n",
    "# estMatrix includes estimated ratings of one algorithm as a column\n",
    "estMatrix = np.column_stack((svdEst,nmfEst,randomEst,kNNEst,baselineEst)) # Stack 1-D arrays as columns into a 2-D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimated weights:  [ 0.30813112  0.20571974 -0.00292919  0.15563319  0.35188168]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Try to find minimum for: w_1*svdEst + w_2*nmfEst + w_3*randomEst + w_4*kNNEst + w_5*baselineEst - actualRating\n",
    "# OR: estMatrix*w - actualRating \n",
    "# use SGDRegressor for learing of the weights (continous valus are permitted)\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "clf = SGDRegressor(loss=\"squared_loss\", penalty=\"l2\", average=True)\n",
    "clf.fit(estMatrix, correctRating)\n",
    "w = clf.coef_\n",
    "print('estimated weights: ', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on validation set for hybrid algorithm:  0.946023515053171\n",
      "This is only  marginally better than the RMSE value for the best algorithm on the validation set.\n"
     ]
    }
   ],
   "source": [
    "# For Comparison purposes print the RMSE on the VALIDATION SET\n",
    "print('RMSE on validation set for hybrid algorithm: ', np.sqrt(sum((estMatrix.dot(w)-correctRating)**2)/len(correctRating)))\n",
    "print('This is only  marginally better than the RMSE value for the best algorithm on the validation set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9585\n",
      "RMSE: 0.9996\n",
      "RMSE: 1.5259\n",
      "RMSE: 1.0102\n",
      "RMSE: 0.9587\n"
     ]
    }
   ],
   "source": [
    "# Test the hybrid algorithm on the testset:\n",
    "# Use algorithms on the test set and add them up with the calculated weights\n",
    "# We print the RMSE for the individual algorithms in the process\n",
    "testPredictSVD = svdAlgo.test(testset)\n",
    "svdTestAccuracy = accuracy.rmse(testPredictSVD)\n",
    "testPredictNMF = nmfAlgo.test(testset)\n",
    "nmfTestAccuracy = accuracy.rmse(testPredictNMF)\n",
    "testPredictRandom = randomAlgo.test(testset)\n",
    "randomTestAccuracy = accuracy.rmse(testPredictRandom)\n",
    "testPredictkNN = kNNAlgo.test(testset)\n",
    "kNNTestAccuracy = accuracy.rmse(testPredictkNN)\n",
    "testPredictBaseline = baselineAlgo.test(testset)\n",
    "baselineTestAccuracy = accuracy.rmse(testPredictBaseline)\n",
    "\n",
    "svdTestEst = [x.est for x in testPredictSVD]\n",
    "nmfTestEst = [x.est for x in testPredictNMF]\n",
    "randomTestEst = [x.est for x in testPredictRandom]\n",
    "kNNTestEst = [x.est for x in testPredictkNN]\n",
    "baselineTestEst = [x.est for x in testPredictBaseline ]\n",
    "estMatrix = np.column_stack((svdTestEst,nmfTestEst,randomTestEst,kNNTestEst,baselineTestEst))\n",
    "\n",
    "correctTestRating = [x.r_ui for x in testPredictBaseline ]\n",
    "hybridTestEst = estMatrix.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test set for hybrid algorith: 0.9541404468456874\n",
      "This - again - is only a little bit better than the RMSE value for the best algorithm on the test set.\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "print('RMSE on test set for hybrid algorith:', np.sqrt(np.sum((hybridTestEst-correctTestRating)**2)/len(correctTestRating)))\n",
    "print('This - again - is only a little bit better than the RMSE value for the best algorithm on the test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hidden Markov model\n",
    "Consider the HMM package https://hmmlearn.readthedocs.io/en/latest/\n",
    "\n",
    "a. Generate sequences with multinomial HMM (2 symbols and 4 hidden states) and given parameters. Start probability - {0.4,0.2,0.1,0.3}, Transition matrix - {{0.2,0.3,0.1,0.4},{0.3,0.3,0.2,0.2},{0.4,0.2,0.3,0.1},{0.2,0.3,0.1,0.4}}, Emission probability - {{0.2,0.8},{0.1,0.9},{0.5,0.5},{0.6,0.4}}.\n",
    "\n",
    "\n",
    "b. Consider a sequence - {1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1}. Fit a multinomial HMM considering 4 states and obtain hidden state which is most likely to have generated the symbol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled state sequence:  [0 2 0 2 3 0 3 0 3 1 0 0 3 1 0 2 1 2 0 1 2 0 0 0 3 0 1 1 0 1 3 0 3 3 1 0 3\n",
      " 0 3 2 2 0 0 1 1 2 3 3 3 1 1 1 2 0 1 0 1 1 1 1 2 0 3 1 0 1 0 3 1 1 0 3 1 0\n",
      " 1 2 1 0 0 3 3 1 0 3 1 1 3 3 1 1 1 3 2 3 3 1 0 2 0 1]\n",
      "with feature sequence:  [1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 1 1\n",
      " 0 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# -- 3a\n",
    "# scikit-learn HMM module:\n",
    "#     used http://scikit-learn.sourceforge.net/stable/modules/hmm.html for orentation\n",
    "\n",
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "\n",
    "startprobability = np.array([0.4, 0.2, 0.1, 0.3])\n",
    "transmatrix = np.array([[0.2, 0.3, 0.1, 0.4], [0.3, 0.3, 0.2, 0.2], [0.4, 0.2, 0.3, 0.1],[0.2, 0.3, 0.1, 0.4]])\n",
    "emissionprobability = np.array([[0.2,0.8],[0.1,0.9],[0.5,0.5],[0.6,0.4]])\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=4)\n",
    "model.n_symbols = 2\n",
    "model.startprob_ = startprobability\n",
    "model.transmat_ = transmatrix\n",
    "model.emissionprob_ = emissionprobability\n",
    "\n",
    "X,Z = model.sample(100)\n",
    "print('sampled state sequence: ',Z)\n",
    "print('with feature sequence: ', X.transpose()[0]) # transposed for readbility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  31  number of features:  1\n",
      "predicted state sequence:  [0 2 1 2 0 3 1 0 2 0 2 0 2 0 2 0 3 0 2 1 2 1 0 2 0 2 3 1 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "# -- 3b\n",
    "# Fit a HMM on a given sequence and obtain hidden state, which generated the symbol - {1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1}\n",
    "\n",
    "model2 = hmm.MultinomialHMM(n_components=4)\n",
    "#model2.n_symbols = 2 should be assumed\n",
    "X = np.array([[1], [0], [0], [0], [1], [1], [1], [1], [0], [1], [0], [1], [0], [1], [0], [1], [1], [1], [0], [0], [0], [1], [1], [0], [1], [0], [0], [1], [1], [0], [1]])\n",
    "print('number of samples: ', X.shape[0], ' number of features: ', X.shape[1])\n",
    "model2.fit(X) \n",
    "Z2 = model2.predict(X)\n",
    "print('predicted state sequence: ', Z2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PrefixSpan \n",
    "Implement the Prefix algorithm for Sequential Pattern Mining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with influences from: http://sequenceanalysis.github.io/\n",
    "# but tried to implement independently (since I looked at the code from github first, the structure is pretty similar...)\n",
    "\n",
    "# example input of a sequence database\n",
    "# list of sequences (sequences can contain another list of items that occur at the same time)\n",
    "db = [\n",
    "    [[\"a\", \"b\"], [\"c\"], [\"d\"], [\"e\"]], # here a and b occur at the same time at time step 0\n",
    "    [[\"b\"], [\"b\"], [\"b\", \"d\"], [\"e\"]],\n",
    "    [[\"c\"], [\"b\"], [\"c\"], [\"c\"], [\"a\"]],\n",
    "    [[\"b\"], [\"b\"], [\"b\", \"c\", \"c\"]],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    write sequence pattern as a string\n",
    "    called to save support count in a dict\n",
    "\"\"\"\n",
    "def toStr(seqPattern):\n",
    "    result = \"[\"\n",
    "    #print(seqPattern)\n",
    "    for j in range(len(seqPattern)):\n",
    "        result = result + \"[\"\n",
    "        for i in range(len(seqPattern[j])):\n",
    "            result = result + seqPattern[j][i]\n",
    "            if i!=len(seqPattern[j])-1:\n",
    "                result = result + \",\"\n",
    "        result = result + \"]\"\n",
    "        if j!=len(seqPattern)-1:\n",
    "            result = result + \",\"\n",
    "    result = result + \"]\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[a,b],[c]]'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toStr([[\"a\",\"b\"],[\"c\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" project whole database to prefix\n",
    "\n",
    "    prefix: is one timestep of a sequence\n",
    "        here: [[]]-notation, but only ever called with one time step!\n",
    "        I implemented this before I knew this is the case -> does more than it has to, and also more complicated...\n",
    "        what it does more: projects to a prefix, which can consist of several time steps!\n",
    "    \n",
    "    if nextTimestep is true: jump over first timestep since prefix is supposed to happen in a new timestep\n",
    "    if nextTimestep is false: can happen at the same timestep\n",
    "    result: projected db: does not replace prefix with '_' as in the algorithm \n",
    "\"\"\"\n",
    "def projectDatabase(database, prefix, nextTimestep=False):\n",
    "    projectedDB = []\n",
    "    for sequence in database:\n",
    "        if (nextTimestep): # prefix supposed to happen in the next time step\n",
    "            sequence = sequence[1:]\n",
    "        \n",
    "        # ********** project sequence ************* #\n",
    "        sequenceProjection = []\n",
    "\n",
    "        for startLeft in range(len(sequence)): # shift starting point of prefix\n",
    "            if (startLeft+len(prefix)>len(sequence)): break # break if prefix does not fit after startLeft\n",
    "            sequenceProjection = copy.deepcopy(sequence)[startLeft:] # copy sequence from startLeft to projection\n",
    "            \n",
    "            if not all(x in sequence[startLeft] for x in prefix[0]):\n",
    "                sequenceProjection = []\n",
    "                continue # if first time step of prefix is not contained in sequence that starts at startLeft, increase startLeft\n",
    "\n",
    "            j = startLeft # j is current position that is checked against prefix time step i\n",
    "            for i in range(len(prefix)-1): # this is acutally never called\n",
    "                while(j<len(sequence)):\n",
    "                    if not all(x in sequence[j+i+1] for x in prefix[i+1]): # check if prefix is contained in sequence\n",
    "                        j+=1 # look for prefix timestep in next sequence timestep\n",
    "                    else:\n",
    "                        break\n",
    "                if j==len(sequence):\n",
    "                    sequenceProjection = []\n",
    "                    break\n",
    "                        \n",
    "            if len(sequenceProjection)>0:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        if len(sequenceProjection)>0:\n",
    "            projectedDB.append(sequenceProjection)\n",
    "            \n",
    "    return projectedDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "validateProjectDB = [ [[\"a\"], [\"b\", \"c\"], [\"a\", \"c\"], [\"c\"]],[['a']] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a', 'c'], ['c']]]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectDatabase(validateProjectDB, [\"a\"], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a'], ['b', 'c'], ['a', 'c'], ['c']]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectDatabase(validateProjectDB,[[\"a\"],[\"b\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a'], ['c'], ['a']]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projectDatabase([ [[\"b\"], [\"a\"], [\"c\"], [\"a\"]] ], [[\"a\"],[\"a\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['a', 'b'], ['c'], ['d'], ['e']], [['a']]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdb = projectDatabase(db,[['a']])\n",
    "testdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    get all frequent Items from a database (chars), and their support\n",
    "    newEvent: ignores the first timestep\n",
    "    prefix: only consider one timestep!! (only relevant if countInNewEvent is false, i.e. second part of recursion)\n",
    "'''\n",
    "\n",
    "from collections import defaultdict #https://www.accelebrate.com/blog/using-defaultdict-python/\n",
    "# support: number of sequences that have the items as a subsequence! (does not count occurences within sequence)\n",
    "def getFrequentItems(database, min_sup, prefix=[], countInNewEvent=False):\n",
    "    supports = defaultdict(int) # initiates every key with 0 --> no key errors\n",
    "    # set countWithinTimestep, i.e. if it is supposed to happen simultaneously with prefix\n",
    "    countWithinTimestep = False\n",
    "    if (len(prefix)>0 and not countInNewEvent):\n",
    "        countWithinTimestep = True\n",
    "    #print('countWithinTimestep: ', countWithinTimestep)\n",
    "    \n",
    "    for sequence in database:\n",
    "        if countInNewEvent: # item is supposed to happen in a new time step -> jump over first timestep\n",
    "            sequence = sequence[1:]\n",
    "\n",
    "        uniqueItems = set() # set saves items that occur in a sequence\n",
    "        for timestep in sequence:\n",
    "            if (countWithinTimestep):\n",
    "                if all(x in timestep for x in prefix):\n",
    "                    #print('prefix: ',prefix, ' and timestep: ', timestep)\n",
    "                    #print('prefix in timestep: ', all(x in timestep for x in prefix))\n",
    "                    for item in timestep:\n",
    "                        if (not (item in prefix)):\n",
    "                            uniqueItems.add(item)\n",
    "            else:\n",
    "                for item in timestep:\n",
    "                    uniqueItems.add(item)\n",
    "    \n",
    "        #print('unique Item set:', uniqueItems, ' in timestep: ', timestep)\n",
    "        for item in uniqueItems: #max support for every item: number of sequences\n",
    "            supports[item] +=1\n",
    "            \n",
    "    result = defaultdict(int)\n",
    "    for it, support in supports.items():\n",
    "        if (support>=min_sup):\n",
    "            result[it] = support\n",
    "    return sorted(result.items()) # make sure the '>'-cond in prefixSpan works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('c', 3)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFrequentItems(db, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 1), ('d', 1), ('e', 1)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFrequentItems(testdb, 0, ['a'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" high level algorithm call\n",
    "    input: sequence database, minimum support\n",
    "    output: dict of sequential patterns : count\n",
    "\"\"\"\n",
    "def prefixSpan(sequence_DB,min_sup):\n",
    "    result = {}\n",
    "    \n",
    "    frequentItems = getFrequentItems(sequence_DB, min_sup)\n",
    "    for (item, support) in frequentItems:\n",
    "        prefix = [[item]]\n",
    "        result[toStr(prefix)] = support\n",
    "        result.update( prefixSpanRecursion(prefix,projectDatabase(sequence_DB,prefix), min_sup) )\n",
    "    return result\n",
    "\n",
    "\"\"\" \n",
    "    recursive call of prefixspan\n",
    "    input: projected sequence db, minimum support, current prefix\n",
    "\"\"\"\n",
    "def prefixSpanRecursion(prefix,current_DB, min_sup):\n",
    "    result = {}\n",
    "    #print('call of recursion with prefix: ', prefix, 'and db: ', current_DB)\n",
    "    \n",
    "    # \"assemble to last element of prefix\": add to last timestep (happens at the same time)\n",
    "    frequentItemsSameEvent = getFrequentItems(current_DB,min_sup,prefix[-1], False)\n",
    "    #print('frequent item same event: ', frequentItemsSameEvent)\n",
    "    for (item, support) in frequentItemsSameEvent:\n",
    "        if all(item>x for x in prefix[-1]):\n",
    "            #print('item: ', item, 'support: ', support)\n",
    "            #print('prefix before: ', prefix)\n",
    "            newPrefix = copy.deepcopy(prefix)\n",
    "            newPrefix[-1].append(item)\n",
    "            #print('new prefix:', newPrefix, 'count:',support )\n",
    "            result[toStr(newPrefix)] = support\n",
    "            result.update(prefixSpanRecursion(newPrefix, projectDatabase(current_DB,[newPrefix[-1]], False),min_sup))\n",
    "\n",
    "    # \"append to prefix\": add new timestep\n",
    "    frequentItemsNewEvent = getFrequentItems(current_DB,min_sup,prefix[-1], True)\n",
    "    #print('frequentItemsNewEvent: ', frequentItemsNewEvent)\n",
    "    for (item, support) in frequentItemsNewEvent: \n",
    "        newPrefixB = copy.deepcopy(prefix)\n",
    "        newPrefixB.append([item])\n",
    "        #print('new prefix:', newPrefixB, 'count:',support )\n",
    "        result[toStr(newPrefixB)] = support\n",
    "        result.update(prefixSpanRecursion(newPrefixB, projectDatabase(current_DB,[newPrefixB[-1]], True),min_sup))\n",
    "         \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetValidate = [\n",
    "    [[\"a\"], [\"a\", \"b\", \"c\"], [\"a\", \"c\"], [\"c\"]],\n",
    "    [[\"a\"], [\"c\"], [\"b\", \"c\"]],\n",
    "    [[\"a\", \"b\"], [\"d\"], [\"c\"], [\"b\"], [\"c\"]],\n",
    "    [[\"a\"], [\"c\"], [\"b\"], [\"c\"]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prefixSpan(datasetValidate, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[[a,b],[c],[c]]': 2,\n",
       " '[[a,b],[c]]': 2,\n",
       " '[[a,b]]': 2,\n",
       " '[[a],[b,c]]': 2,\n",
       " '[[a],[b],[c]]': 3,\n",
       " '[[a],[b]]': 4,\n",
       " '[[a],[c],[b],[c]]': 2,\n",
       " '[[a],[c],[b]]': 3,\n",
       " '[[a],[c],[c]]': 4,\n",
       " '[[a],[c]]': 4,\n",
       " '[[a]]': 4,\n",
       " '[[b,c]]': 2,\n",
       " '[[b],[c],[c]]': 2,\n",
       " '[[b],[c]]': 3,\n",
       " '[[b]]': 4,\n",
       " '[[c],[b],[c]]': 2,\n",
       " '[[c],[b]]': 3,\n",
       " '[[c],[c]]': 4,\n",
       " '[[c]]': 4}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result # matches results from algorithm found here: http://sequenceanalysis.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
